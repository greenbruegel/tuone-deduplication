{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0ae0bb",
   "metadata": {},
   "source": [
    "# Factory list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e0975a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import networkx as nx\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from openpyxl import load_workbook\n",
    "from neo4j import GraphDatabase\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.styles import Font\n",
    "from copy import copy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id       ID  \\\n",
      "0  67f52a45981040986eab706d  2700008   \n",
      "1  67f52a45981040986eab706e  2700013   \n",
      "2  67f52a45981040986eab706f  2700025   \n",
      "3  67f52a46981040986eab7070  2700027   \n",
      "4  67f52a46981040986eab7071  2700032   \n",
      "\n",
      "                                                 url                 date  \n",
      "0  https://www.electrive.com/2016/01/18/united-ki...  2016-01-18 00:00:00  \n",
      "1  https://www.electrive.com/2016/01/26/baic-chin...  2016-01-26 00:00:00  \n",
      "2  https://www.electrive.com/2016/01/25/bmw-wanxi...  2016-01-25 00:00:00  \n",
      "3  https://www.electrive.com/2016/01/20/morgan-to...  2016-01-20 00:00:00  \n",
      "4  https://www.electrive.com/2016/01/29/volkswage...  2016-01-29 00:00:00  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Connect to MongoDB ===\n",
    "mongo_client = MongoClient(\"\") #to be added\n",
    "mongo_db = mongo_client[\"tuone\"] \n",
    "articles_collection = mongo_db[\"articles\"] # change \"articles\"\n",
    "\n",
    "# === Fetch all documents with _id and meta fields ===\n",
    "docs = list(\n",
    "    articles_collection.find(\n",
    "        {},  # No filter — get all documents\n",
    "        {\n",
    "            \"_id\": 1,\n",
    "            \"meta.ID\": 1,\n",
    "            \"meta.url\": 1,\n",
    "            \"meta.date\": 1\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# === Create DataFrame ===\n",
    "data = [{\n",
    "    \"_id\": str(doc.get(\"_id\")),\n",
    "    \"ID\": doc.get(\"meta\", {}).get(\"ID\"),\n",
    "    \"url\": doc.get(\"meta\", {}).get(\"url\"),\n",
    "    \"date\": doc.get(\"meta\", {}).get(\"date\")\n",
    "} for doc in docs]\n",
    "\n",
    "df_meta = pd.DataFrame(data)\n",
    "\n",
    "# === Display sample or save ===\n",
    "print(df_meta.head())\n",
    "# Optionally save to file:\n",
    "# df_meta.to_csv(\"article_meta_export.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Query articles from MongoDB ===\n",
    "articles_to_process = list(\n",
    "    articles_collection.find(\n",
    "        {\n",
    "            \"nodes\": {\"$exists\": True},\n",
    "            \"relationships\": {\"$exists\": True}\n",
    "        },\n",
    "        {\n",
    "            \"_id\": 1,\n",
    "            \"meta.ID\": 1,\n",
    "            \"nodes\": 1,\n",
    "            \"relationships\": 1\n",
    "        }\n",
    "    ).sort(\"meta.ID\", 1)\n",
    ")\n",
    "\n",
    "# === Flatten helper ===\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            v = None if isinstance(v, str) and v.lower() == \"null\" else v\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "# === Process nodes and relationships ===\n",
    "all_nodes = []\n",
    "all_rels = []\n",
    "\n",
    "for doc in articles_to_process:\n",
    "    article_id = doc.get(\"meta\", {}).get(\"ID\", None)\n",
    "\n",
    "    # === Nodes ===\n",
    "    df_nodes = pd.DataFrame(doc.get(\"nodes\", []))\n",
    "    for _, row in df_nodes.iterrows():\n",
    "        node_id = row.get(\"id\")\n",
    "        label = row.get(\"type\", \"Entity\")\n",
    "        if not node_id or not label:\n",
    "            continue\n",
    "        raw_props = {k: v for k, v in row.items() if k not in [\"id\", \"type\"]}\n",
    "        flat_props = flatten_dict(raw_props)\n",
    "        flat_props.update({\n",
    "            \"article_id\": article_id,\n",
    "            \"id\": node_id,\n",
    "            \"label\": label\n",
    "        })\n",
    "        all_nodes.append(flat_props)\n",
    "\n",
    "    # === Relationships ===\n",
    "    df_rels = pd.DataFrame(doc.get(\"relationships\", []))\n",
    "    for _, row in df_rels.iterrows():\n",
    "        source = row.get(\"source\")\n",
    "        target = row.get(\"target\")\n",
    "        rel_type = row.get(\"type\", \"RELATED_TO\")\n",
    "        group = row.get(\"group\", \"unspecified\")\n",
    "        if not source or not target:\n",
    "            continue\n",
    "        raw_props = {k: v for k, v in row.items() if k not in [\"source\", \"target\", \"type\", \"group\"]}\n",
    "        flat_props = flatten_dict(raw_props)\n",
    "        flat_props.update({\n",
    "            \"article_id\": article_id,\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "            \"type\": rel_type,\n",
    "            \"group\": group\n",
    "        })\n",
    "        all_rels.append(flat_props)\n",
    "\n",
    "# === Convert to DataFrames ===\n",
    "df_all_nodes = pd.DataFrame(all_nodes)\n",
    "df_all_rels = pd.DataFrame(all_rels)\n",
    "\n",
    "print(\"Loaded to DataFrames: nodes =\", len(df_all_nodes), \"| relationships =\", len(df_all_rels))\n",
    "\n",
    "# === Create unique_id for each node ===\n",
    "df_all_nodes[\"unique_id\"] = df_all_nodes[\"article_id\"] + \"_\" + df_all_nodes[\"id\"]\n",
    "df_all_nodes_raw = df_all_nodes.copy()\n",
    "\n",
    "# === Create lookup dictionaries ===\n",
    "id_to_unique = df_all_nodes.set_index(['article_id', 'id'])['unique_id'].to_dict()\n",
    "id_to_label = df_all_nodes.set_index(['article_id', 'id'])['label'].to_dict()\n",
    "\n",
    "# === Map source and target to unique_id and label ===\n",
    "def get_unique_id(row, which):\n",
    "    return id_to_unique.get((row['article_id'], row[which]))\n",
    "\n",
    "def get_label(row, which):\n",
    "    return id_to_label.get((row['article_id'], row[which]))\n",
    "\n",
    "df_all_rels['source_label'] = df_all_rels.apply(lambda row: get_label(row, 'source'), axis=1)\n",
    "df_all_rels['target_label'] = df_all_rels.apply(lambda row: get_label(row, 'target'), axis=1)\n",
    "df_all_rels['source'] = df_all_rels.apply(lambda row: get_unique_id(row, 'source'), axis=1)\n",
    "df_all_rels['target'] = df_all_rels.apply(lambda row: get_unique_id(row, 'target'), axis=1)\n",
    "df_all_rels_raw = df_all_rels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "433bcb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved factory-centric outputs to reconciliation_outputs_factory.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Helper Functions ===\n",
    "def extract_ids(x):\n",
    "    if isinstance(x, list):\n",
    "        return [i.split(\"_\")[1] if \"_\" in i else i for i in x]\n",
    "    return []\n",
    "\n",
    "def safe_lookup_list(ids, lookup, key):\n",
    "    if not isinstance(ids, list):\n",
    "        return []\n",
    "    result = [lookup.get(i, {}).get(key, None) for i in ids]\n",
    "    while len(result) < len(ids):\n",
    "        result.append(None)\n",
    "    return result[:len(ids)]\n",
    "\n",
    "def build_reconciliation_lookup(log):\n",
    "    df_log = pd.DataFrame(log)\n",
    "    df_log[\"article_id\"] = df_log[\"original_unique_id\"].str.extract(r\"(^[^_]+)\")\n",
    "    product_ids = df_log[df_log[\"entity_type\"] == \"product\"].groupby(\"new_unique_id\")[\"article_id\"].apply(set).to_dict()\n",
    "    company_ids = df_log[df_log[\"entity_type\"] == \"company\"].groupby(\"new_unique_id\")[\"article_id\"].apply(set).to_dict()\n",
    "    jv_ids = df_log[df_log[\"entity_type\"] == \"joint_venture\"].groupby(\"new_unique_id\")[\"article_id\"].apply(set).to_dict()\n",
    "    investment_ids = df_log[df_log[\"entity_type\"] == \"investment\"].groupby(\"new_unique_id\")[\"article_id\"].apply(set).to_dict()\n",
    "    capacity_ids = df_log[df_log[\"entity_type\"] == \"capacity\"].groupby(\"new_unique_id\")[\"article_id\"].apply(set).to_dict()\n",
    "\n",
    "    for uid in df_log[\"new_unique_id\"]:\n",
    "        fallback = {uid.split(\"_\")[0]}\n",
    "        if uid.startswith(\"product_\"):\n",
    "            product_ids.setdefault(uid, fallback)\n",
    "        elif uid.startswith(\"company_\"):\n",
    "            company_ids.setdefault(uid, fallback)\n",
    "        elif uid.startswith(\"investment_\"):\n",
    "            investment_ids.setdefault(uid, fallback)\n",
    "        elif uid.startswith(\"capacity_\"):\n",
    "            capacity_ids.setdefault(uid, fallback)\n",
    "        elif uid.startswith(\"joint_venture_\"):\n",
    "            jv_ids.setdefault(uid, fallback)\n",
    "\n",
    "    return df_log, capacity_ids, product_ids, company_ids, jv_ids, investment_ids\n",
    "\n",
    "def make_article_id_to_url(df_meta):\n",
    "    df_meta[\"ID\"] = df_meta[\"ID\"].astype(str)\n",
    "    return df_meta.set_index(\"ID\")[\"url\"].to_dict()\n",
    "\n",
    "def resolve_urls(article_ids, article_id_to_url):\n",
    "    if not isinstance(article_ids, (list, set)):\n",
    "        return []\n",
    "    return [article_id_to_url[aid] for aid in article_ids if aid in article_id_to_url]\n",
    "\n",
    "def resolve_urls_from_uid(uids, article_id_to_url):\n",
    "    if not isinstance(uids, list):\n",
    "        return []\n",
    "    return [article_id_to_url.get(uid[:7]) for uid in uids if isinstance(uid, str) and uid[:7] in article_id_to_url]\n",
    "\n",
    "\n",
    "def deduplicate_nodes_and_rels(df_nodes, df_rels):\n",
    "    return (\n",
    "        df_nodes.drop_duplicates(subset=\"unique_id\"),\n",
    "        df_rels.drop_duplicates(subset=[\"source\", \"target\", \"type\"])\n",
    "    )\n",
    "\n",
    "def extract_node_subsets(df_nodes):\n",
    "    return {\n",
    "        \"joint_ventures\": df_nodes[df_nodes[\"label\"].str.lower() == \"joint_venture\"],\n",
    "        \"factories\": df_nodes[df_nodes[\"label\"].str.lower().str.contains(\"factory\", na=False)].copy(),\n",
    "        \"capacities\": df_nodes[df_nodes[\"label\"].str.lower() == \"capacity\"],\n",
    "        \"products\": df_nodes[df_nodes[\"label\"].str.lower() == \"product\"],\n",
    "        \"companies\": df_nodes[df_nodes[\"label\"].str.lower() == \"company\"],\n",
    "        \"investment\": df_nodes[df_nodes[\"label\"].str.lower() == \"investment\"]\n",
    "    }\n",
    "\n",
    "def extract_relationship_subsets(df_rels):\n",
    "    return {\n",
    "        \"owns\": df_rels[df_rels[\"type\"].str.lower() == \"owns\"],\n",
    "        \"at\": df_rels[df_rels[\"type\"].str.lower() == \"at\"],\n",
    "        \"produced_at\": df_rels[df_rels[\"type\"].str.lower() == \"produced_at\"],\n",
    "        \"funds\": df_rels[df_rels[\"type\"].str.lower() == \"funds\"]\n",
    "    }\n",
    "\n",
    "def group_linked_nodes(rel_df, source_nodes, source_col, target_col, entity_label_prefix):\n",
    "    rel_df = rel_df.copy()\n",
    "    df = rel_df.rename(columns={\"source\": source_col, \"target\": target_col})\n",
    "    df = df.merge(\n",
    "        source_nodes[[\"unique_id\", \"name\"]].rename(columns={\n",
    "            \"unique_id\": f\"{entity_label_prefix}_unique_id\",\n",
    "            \"name\": f\"{entity_label_prefix}_name\"\n",
    "        }),\n",
    "        left_on=source_col, right_on=f\"{entity_label_prefix}_unique_id\", how=\"left\"\n",
    "    )\n",
    "    grouped = df.groupby(target_col).agg({\n",
    "        f\"{entity_label_prefix}_unique_id\": lambda x: list(x.dropna().unique()),\n",
    "        f\"{entity_label_prefix}_name\": lambda x: list(x.dropna().unique())\n",
    "    }).reset_index().rename(columns={target_col: \"factory_unique_id\"})\n",
    "    return grouped\n",
    "\n",
    "\n",
    "\n",
    "def run_factory_centric_enrichment(df_all_nodes, df_all_rels, df_meta):\n",
    "    # capacity_article_ids, product_article_ids, company_article_ids, joint_venture_article_ids, investment_article_ids = build_reconciliation_lookup(main_reconciliation_log)\n",
    "    article_id_to_url = make_article_id_to_url(df_meta)\n",
    "    df_all_nodes, df_all_rels = deduplicate_nodes_and_rels(df_all_nodes, df_all_rels)\n",
    "    nodes = extract_node_subsets(df_all_nodes)\n",
    "    rels = extract_relationship_subsets(df_all_rels)\n",
    "\n",
    "    # Extract city and country from already flattened factory nodes\n",
    "    df_factory_locations = nodes[\"factories\"][[\"name\", \"unique_id\", \"location_city\", \"location_country\"]].rename(\n",
    "        columns={\n",
    "            \"unique_id\": \"factory_unique_id\",\n",
    "            \"location_city\": \"factory_city\",\n",
    "            \"location_country\": \"factory_country\",\n",
    "            \"name\":\"factory_name\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    df_owns_comp = group_linked_nodes(rels[\"owns\"], nodes[\"companies\"], \"source\", \"target\", \"owner_company\")\n",
    "    df_owns_jv = group_linked_nodes(rels[\"owns\"], nodes[\"joint_ventures\"], \"source\", \"target\", \"owner_jv\")\n",
    "    df_funds = group_linked_nodes(rels[\"funds\"], nodes[\"investment\"], \"source\", \"target\", \"investment\")\n",
    "    df_products = group_linked_nodes(rels[\"produced_at\"], nodes[\"products\"], \"source\", \"target\", \"product\")\n",
    "    df_capacities = group_linked_nodes(rels[\"at\"], nodes[\"capacities\"], \"source\", \"target\", \"capacity\")\n",
    "\n",
    "    df_master = reduce(lambda left, right: pd.merge(left, right, on=\"factory_unique_id\", how=\"outer\"), [\n",
    "        df_owns_comp, df_owns_jv, df_funds, df_products, df_capacities\n",
    "    ])\n",
    "    # Merge city and country info\n",
    "    df_master = df_master.merge(df_factory_locations, on=\"factory_unique_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    inv_lookup = nodes[\"investment\"].set_index(\"unique_id\")[[\"name\", \"status\", \"amount\", \"phase\"]].to_dict(\"index\")\n",
    "    cap_lookup = nodes[\"capacities\"].set_index(\"unique_id\")[[\"name\", \"status\", \"amount\", \"phase\"]].to_dict(\"index\")\n",
    "\n",
    "    df_master[\"investment_name\"] = df_master[\"investment_unique_id\"].apply(lambda uids: safe_lookup_list(uids, inv_lookup, \"name\"))\n",
    "    df_master[\"investment_status\"] = df_master[\"investment_unique_id\"].apply(lambda uids: safe_lookup_list(uids, inv_lookup, \"status\"))\n",
    "    df_master[\"investment_amount\"] = df_master[\"investment_unique_id\"].apply(lambda uids: safe_lookup_list(uids, inv_lookup, \"amount\"))\n",
    "    df_master[\"investment_phase\"] = df_master[\"investment_unique_id\"].apply(lambda uids: safe_lookup_list(uids, inv_lookup, \"phase\"))\n",
    "\n",
    "    df_master[\"capacity_name\"] = df_master[\"capacity_unique_id\"].apply(lambda ids: safe_lookup_list(ids, cap_lookup, \"name\"))\n",
    "    df_master[\"capacity_status\"] = df_master[\"capacity_unique_id\"].apply(lambda ids: safe_lookup_list(ids, cap_lookup, \"status\"))\n",
    "    df_master[\"capacity_amount\"] = df_master[\"capacity_unique_id\"].apply(lambda ids: safe_lookup_list(ids, cap_lookup, \"amount\"))\n",
    "    df_master[\"capacity_phase\"] = df_master[\"capacity_unique_id\"].apply(lambda ids: safe_lookup_list(ids, cap_lookup, \"phase\"))\n",
    "\n",
    "\n",
    "    df_master[\"factory_urls\"] = df_master[\"factory_unique_id\"].apply(lambda uids: resolve_urls_from_uid(uids, article_id_to_url))\n",
    "    df_master[\"product_urls\"] = df_master[\"product_unique_id\"].apply(lambda uids: resolve_urls_from_uid(uids, article_id_to_url))\n",
    "    df_master[\"capacity_urls\"] = df_master[\"capacity_unique_id\"].apply(lambda uids: resolve_urls_from_uid(uids, article_id_to_url))\n",
    "    df_master[\"owner_company_urls\"] = df_master[\"owner_company_unique_id\"].apply(lambda uids: resolve_urls_from_uid(uids, article_id_to_url))\n",
    "    df_master[\"owner_jv_urls\"] = df_master[\"owner_jv_unique_id\"].apply(lambda uids: resolve_urls_from_uid(uids, article_id_to_url))\n",
    "    df_master[\"investment_urls\"] = df_master[\"investment_unique_id\"].apply(lambda uids: resolve_urls_from_uid(uids, article_id_to_url))\n",
    "\n",
    "    # df_master[\"factory_article_ids\"] = df_master[\"factory_unique_id\"].apply(extract_ids)\n",
    "    # df_master[\"factory_urls\"] = df_master[\"factory_article_ids\"].apply(lambda ids: resolve_urls(ids, article_id_to_url))\n",
    "\n",
    "    # df_master[\"product_article_ids\"] = df_master[\"product_unique_id\"].apply(extract_ids)\n",
    "    # df_master[\"product_urls\"] = df_master[\"product_article_ids\"].apply(lambda ids: resolve_urls(ids, article_id_to_url))\n",
    "\n",
    "    # df_master[\"capacity_article_ids\"] = df_master[\"capacity_unique_id\"].apply(extract_ids)\n",
    "    # df_master[\"capacity_urls\"] = df_master[\"capacity_article_ids\"].apply(lambda ids: resolve_urls(ids, article_id_to_url))\n",
    "\n",
    "    # df_master[\"company_article_ids\"] = df_master[\"owner_company_unique_id\"].apply(\n",
    "    #     lambda uids: [aid for uid in uids for aid in company_article_ids.get(uid, [])] if isinstance(uids, list) else [])\n",
    "    # df_master[\"company_urls\"] = df_master[\"company_article_ids\"].apply(lambda ids: resolve_urls(ids, article_id_to_url))\n",
    "\n",
    "    # df_master[\"joint_venture_article_ids\"] = df_master[\"owner_jv_unique_id\"].apply(\n",
    "    #     lambda uids: [aid for uid in uids for aid in joint_venture_article_ids.get(uid, [])] if isinstance(uids, list) else [])\n",
    "    # df_master[\"joint_venture_urls\"] = df_master[\"joint_venture_article_ids\"].apply(lambda ids: resolve_urls(ids, article_id_to_url))\n",
    "\n",
    "    df_master_final = df_master[[\n",
    "        \"factory_name\", \"factory_country\", \"factory_city\", \"factory_urls\",\n",
    "        \"owner_company_name\", \n",
    "        \"owner_jv_name\", \n",
    "        \"product_name\", \n",
    "        \"capacity_name\", \"capacity_status\",  \"capacity_phase\", \"capacity_amount\",\n",
    "        \"investment_name\", \"investment_status\", \"investment_phase\", \"investment_amount\", \n",
    "    ]]\n",
    "\n",
    "\n",
    "    # df_canonical = pd.DataFrame([{\"entity_type\": k, \"unique_id\": v} for k, vs in canonical_entities.items() for v in vs])\n",
    "\n",
    "    df_factories_pivot = df_master.explode([\"factory_unique_id\"])\n",
    "    df_owner_companies_pivot = df_master.explode(\"owner_company_name\")\n",
    "    df_owner_jvs_pivot = df_master.explode(\"owner_jv_name\")\n",
    "    df_products_pivot = df_master.explode([\"product_name\"])\n",
    "    df_capacities_pivot = df_master.explode([\"capacity_name\", \"capacity_status\", \"capacity_amount\", \"capacity_phase\"])\n",
    "    df_investments_pivot = df_master.explode([\"investment_name\", \"investment_status\", \"investment_amount\", \"investment_phase\"])\n",
    "\n",
    "    with pd.ExcelWriter(\"reconciliation_outputs_factory.xlsx\", engine=\"openpyxl\") as writer:\n",
    "        df_master.to_excel(writer, sheet_name=\"factory\", index=False)\n",
    "        df_master_final.to_excel(writer, sheet_name=\"summary_view_factory\", index=False)\n",
    "        df_factories_pivot.to_excel(writer, sheet_name=\"pivot_factories\", index=False)\n",
    "        df_owner_companies_pivot.to_excel(writer, sheet_name=\"pivot_owner_companies\", index=False)\n",
    "        df_owner_jvs_pivot.to_excel(writer, sheet_name=\"pivot_owner_jvs\", index=False)\n",
    "        df_products_pivot.to_excel(writer, sheet_name=\"pivot_products\", index=False)\n",
    "        df_capacities_pivot.to_excel(writer, sheet_name=\"pivot_capacities\", index=False)\n",
    "        df_investments_pivot.to_excel(writer, sheet_name=\"pivot_investments\", index=False)\n",
    "        # df_reconcile_log.to_excel(writer, sheet_name=\"reconciliation_log_factory\", index=False)\n",
    "        # df_canonical.to_excel(writer, sheet_name=\"canonical_entities_factory\", index=False)\n",
    "\n",
    "    print(\"\\u2705 Saved factory-centric outputs to reconciliation_outputs_factory.xlsx\")\n",
    "\n",
    "# === Sample Data ===\n",
    "# Run enrichment\n",
    "run_factory_centric_enrichment(df_all_nodes, df_all_rels, df_meta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
